---
layout: post
title: Spark-Spark SQL
category: study
tags: 
description:
---

## Introduction
- Spark SQL的应用不局限于sql
- 访问hive,json,parquet等文件的数据
- sql知识spark SQL的一个功能
- Spark SQL提供了SQL的api,DataFrame和Dataset的API

## Architecture
![Spark]({{site.url}}/assets/image/interview/30.png)

## Spark Entrance

### SQLContext

### HiveContext

### SparkSession
1.修改scala版本2.11.8 spark2.1.0 

```
vim pom.xml
 <properties>
    <scala.version>2.11.8</scala.version>
    <spark.version>2.1.0</spark.version>
  </properties>

<dependency>
    <groupId>org.scala-lang</groupId>
    <artifactId>scala-library</artifactId>
    <version>${scala.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>${spark.version}</version>
</dependency>
<!--访问thriftserver-->
<dependency>
    <groupId>org.spark-project.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <version>1.2.1.spark2</version>
</dependency>

vim people
{"name": "newcome","age":12}
{"name": "xiaoming","age":12}
{"age":12}
```

```
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("SparkSessionApp").master("local[2]").getOrCreate()
    //返回的是一个DataFrame
    val people = spark.read.json("file:///Users/newcome/development/devtmp/people")
    people.show()
    spark.stop()
  }
```

## Spark Shell
- 复制hive-site.xml到spark目录下

```
vim hive-site.xml
<!--
  <property>
    <name>hive.server2.thrift.client.user</name>
    <value>root</value>
    <description>Username to use against thrift client</description>
  </property>
  <property>
    <name>hive.server2.thrift.client.password</name>
    <value>123456</value>
    <description>Password to use against thrift client</description>
  </property>
-->
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
    <description>Username to use against thrift client</description>
  </property>
```
- 准备 mysql-connector-java-5.1.47.jar
- 访问

```
#mysql 最好默认字符集都为latin1,目前仍需研究字符集

#shell方式启动
./bin/spark-shell --master local[2] --jars /Users/newcome/development/env/spark-2.1.0-bin-2.6.0-cdh5.7.0/lib/mysql-connector-java-5.1.47.jar
spark.sql("show tables").show
spark.sql("select * form dept").show

#sql方式启动
./bin/spark-sql --master local[2] --jars /Users/newcome/development/env/spark-2.1.0-bin-2.6.0-cdh5.7.0/lib/mysql-connector-java-5.1.47.jar
#可访问4040端口web界面
#可看到分析路径
explain select * form dept;
```

## thriftserver和beeline

### construction
- spark-shell spark-sql都是一个spark application
- thriftserver 不管你启动了多少个客户端(beeline/code),永远都是一个spark application解决一个数据共享的问题,多个客户端可以共享数据

```
# 启动thriftserver默认端口是10000
./sbin/start-thriftserver.sh --master local[2] --jars /Users/newcome/development/env/spark-2.1.0-bin-2.6.0-cdh5.7.0/lib/mysql-connector-java-5.1.47.jar --hiveconf hive.server2.thrift.port=10000

# 启动beeline
./bin/beeline -u jdbc:hive2://127.0.0.1:10000  -n root
show tables;
```

### java JDBC

```
 def main(args: Array[String]): Unit = {
    Class.forName("org.apache.hive.jdbc.HiveDriver")

    val conn = DriverManager.getConnection("jdbc:hive2://127.0.0.1:10000","newcome","")
    val pstmt = conn.prepareStatement("select * from emp3")
    val rs = pstmt.executeQuery()
    while(rs.next()){
      println("id" + rs.getInt("empno") +
        "  ename" + rs.getString("ename") +
        "  price" + rs.getString("price") +
        "  mgr" + rs.getString("mgr"))
    }
    rs.close()
    pstmt.close()
    conn.close()
  }
```